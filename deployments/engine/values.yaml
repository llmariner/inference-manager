# Default values for inference-manager-engine.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# +docs:section=Global

# Global settings are used to share the values among LLMariner components.
# inference-manager-engine is a dependency of LLMariner, so global values
# are propagated at its installation.
# Please see https://github.com/llmariner/llmariner/blob/main/deployments/llmariner.
global:
  # Specify object store info to manage data.
  # Currently, only object stores with S3-compatible APIs are supported.
  objectStore:
    s3:
      # The region name.
      region: dummy
      # The bucket name to store data.
      bucket: llmariner

      # Optional endpoint URL for the object store.
      # +docs:property
      # endpointUrl: ""

      # Optional AssumeRole.
      # For more information, see [AssumeRole](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html).
      # +docs:property
      # assumeRole:
      #   roleArn: ""
      #   externalId: ""

  # Optional Secret configration for the object store. If specified,
  # the Secret is loaded as environment variables into the container.
  awsSecret:
    # The secret name.
    # +docs:property
    # name: ""

    # The key name with an access key ID set.
    accessKeyIdKey: accessKeyId
    # The key name with a secret access key set.
    secretAccessKeyKey: secretAccessKey

  # Specify the worker configuration to access the control-plane.
  worker:
    # Configuration of the Secret used for worker authentication when
    # communication with the control plane.
    registrationKeySecret:
      # The secret name. `default-cluster-registration-key` is available
      # when the control-plane and worker-plane are in the same cluster.
      # This Secret is generated by cluster-manager-server as default.
      # For more information, see [Install across Multiple Clusters](https://llmariner.ai/docs/setup/install/multi_cluster_production/).
      name: default-cluster-registration-key
      # The key name with a registration key set.
      key: key

    tls:
      # The flag to enable TLS access to the control-plane.
      enable: false

    # If specified, use this address for accessing the control-plane.
    # This is necessary when installing LLMariner in a multi-cluster mode.
    # For more information, see [Install across Multiple Clusters](https://llmariner.ai/docs/setup/install/multi_cluster_production/).
    controlPlaneAddr: ""

# +docs:section=Engine

# The HTTP port number for the health check.
# +docs:type=number
healthPort: 8081
# The HTTP port number for the metrics check.
# +docs:type=number
metricsPort: 8084

# The address of the inference-manager-server to call inference APIs for workers.
inferenceManagerServerWorkerServiceAddr: inference-manager-server-worker-service-grpc:8082
# The address of the model-manager-server to call model APIs for workers.
modelManagerServerWorkerServiceAddr: model-manager-server-worker-service-grpc:8082

componentStatusSender:
  # The flag to enable sending component status to the cluster-manager-server.
  enable : true
  # The name of the component.
  name: inference-manager-engine
  # initialDelay is the time to wait before starting the sender.
  initialDelay: 1m
  # The interval time to send the component status.
  interval: 15m
  # The address of the cluster-manager-server to call worker services.
  clusterManagerServerWorkerServiceAddr: cluster-manager-server-worker-service-grpc:8082

runtime:
  runtimeImages:
    # The container image of Ollama.
    ollama: mirror.gcr.io/ollama/ollama:0.3.6
    # The container image of vLLM.
    vllm: mirror.gcr.io/vllm/vllm-openai:v0.6.6.post1
    # The container image of NVIDIA Triton Server.
    # Release 2.50.0 (https://github.com/triton-inference-server/server/releases/tag/v2.50.0).
    # We might need to recompile models when we upgrade the version of Triton Inference Server
    # and TensortRT-LLM.
    triton: nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3

  # Kubernetes imagePullPolicy for the runtime Statefulset.
  imagePullPolicy: IfNotPresent

  # Use this for the regular shutdown of all inference runtimes.
  # e.g., during weekends or nighttime.
  # NOTE: This feature runs once for each given scheduled time. Hence,
  # if dynamic-model-loading (server side feature) or autoscaler
  # is enabled, new runtime will be created or scale up even during
  # the shutdown period.
  scheduledShutdown:
    # Specify whether to create a CronJob for shutdown.
    # If enable, The CronJob scales up/down all runtime StatefulSets
    # at the specified time. At scaling-up, the number of replicas
    # will be `.model.default.replicas`
    enable: false

    # Container image settings for the CronJob.
    image:
      # The container image of kubectl.
      name: bitnami/kubectl:latest
      # Kubernetes imagePullPolicy for the kubectl.
      pullPolicy: IfNotPresent

    # The shutdown schedule.
    # For more information, see [CronJob](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs)
    schedule:
      # The schedule of scaling-down, following cron syntax.
      # e.g., "0 17 * * 5" # Every Friday at 5 PM
      scaleDown: ""
      # The schedule of scaling-up, following cron syntax.
      # e.g., "0 9 * * 1" # Every Monday at 9 AM
      scaleUp: ""

      # Optional name of the time zone (e.g., "Etc/UTC").
      # If empty, the kube-controller-manager interprets schedules
      # relative to its host time zone.
      # +docs:property
      # timeZone: ""

model:
  # If you want to control pod and volume availability zone and topologies,
  # please use the `WaitForFirstConsumer` as the `volumeBindingMode` and
  # ensure that your CSI plugin supports this mode.
  # https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
  default:
    # The runtime name to use for inference.
    # +docs:enum=ollama,vllm,triton
    runtimeName: ollama

    resources:
      # The resource requests for the runtime pod.
      # +docs:property
      requests:
        cpu: "1000m"
        memory: "500Mi"
      # The resource limits for the runtime pod.
      # +docs:property
      limits:
        nvidia.com/gpu: 1

      # The persistent volume settings for the runtime. If not specified,
      # the runtime StatefulSet use an emptyDir instead.
      # If `shareWithReplicas` is enabled, all replicas in the StatefulSet
      # share the same persistent volume. Otherwise, a persistent volume
      # is created for each replica.
      #
      # For example:
      # volume:
      #   shareWithReplicas: false
      #   storageClassName: "fast"
      #   size: "100Gi"
      #   accessMode: "ReadWriteOnce"
      #
      # +docs:property
      volume: {}

    # The number of replicas for the runtime StatefulSet.
    # +docs:type=number
    replicas: 1

    preloaded: false

    # Specify the context length. If set 0, the default context length
    # is used.
    # +docs:type=number
    contextLength: 0

    # Optional a list of vLLM extra flags.
    # +docs:property
    # vllmExtraFlags: []

    # The name of a K8s scheduler used by model runtime.
    schedulerName: ""

    # The name of a K8s Runtime Class (https://kubernetes.io/docs/concepts/containers/runtime-class/)
    # used by model runtime. This is set the Runtime Class of Nvidia container runtime if it is not
    # a cluster default.
    containerRuntimeClassName: ""

  # Optional a map of model ID to model settings. It overrides the
  # default settings for the model.
  #
  # For example:
  # sample-model:
  #   runtimeName: ollama
  #   resources:
  #     requests:
  #       cpu: "100m"
  #       memory: "128Mi"
  #     limits:
  #       cpu: "200m"
  #       memory: "256Mi"
  #     volume:
  #       storageClassName: "standard"
  #       size: "1Gi"
  #       accessMode: "ReadWriteOnce"
  #   replicas: 1
  #   preloaded: true
  #   contextLength: 1024
  #
  # +docs:property
  overrides: {}

# Optional a list of model IDs to preload. These models are preloaded
# at the startup time.
#
# For example:
# preloadedModelIds:
# - google/gemma-2b-it-q4_0
# - sentence-transformers/all-MiniLM-L6-v2-f16
#
# +docs:property
# preloadedModelIds: []

# Optional a map of model ID to context length. If not specified, the
# default context length is used.
#
# For example:
# modelContextLengths:
#   google/gemma-2b-it-q4_0: 1024
#
# +docs:property
# modelContextLengths: {}

ollama:
  # Keep models in memory for a long period of time as we don't want
  # end users to hit slowness due to GPU memory loading.
  keepAlive: 96h
  # If set, Ollama attemts to all GPUs in a node.
  # Setting this to true was mentioned in https://github.com/ollama/ollama/issues/5494
  # to support multiple H100s, but this setting didn't help.
  forceSpreading: false
  # If set, debugging mode is enabled.
  debug: false
  # The path to the directory for ollama runners.
  runnersDir: /tmp/ollama-runners
  # The maximun number of ollama requests procssed in parallel.
  # +docs:type=number
  maxConcurrentRequests: 0

autoscaler:
  # If set to true, the request base autoscaler will be enabled.
  enable: false
  # The type of autoscaler.
  # +docs:enum=builtin,keda
  type: builtin

  builtin:
    # The initial delay before starting the autoscaler.
    initialDelay: "12s"
    # The period for calculating the scaling.
    syncPeriod: "2s"
    # The grace period before scaling to zero.
    scaleToZeroGracePeriod: "5m"
    # the window size for metrics.
    # e.g., if it's 5 minutes, we'll use the 5-minute average as the metric.
    metricsWindow: "60s"

    # The default scaling configuration commonly used for runtimes.
    defaultScaler:
      # The per-pod metric value that we target to maintain.
      # Currently, this is the concurrent requests per model runtime.
      # +docs:type=number
      targetValue: 100
      # the maximum number of replicas.
      # e.g., if this is 10, the pod can be scaled up to 10.
      # +docs:type=number
      maxReplicas: 10
      # the minimum number of replicas.
      # e.g., if this is 0, the pod can be scaled down to 0.
      # +docs:type=number
      minReplicas: 1
      # The maximum rate of scaling up.
      # e.g., current replicas is 2 and this rate is 3.0,
      # the pod can be scaled up to 6. (ceil(2 * 3.0) = 6)
      # +docs:type=number
      maxScaleUpRate: 3.0
      # The maximum rate of scaling down.
      # e.g., current replicas is 6 and this rate is 0.5,
      # the pod can be scaled down to 3. (floor(6 * 0.5) = 3)
      # +docs:type=number
      maxScaleDownRate: 0.5

    # Optional a map of runtime name to scaler settings. It overrides the
    # default scaling settings for the runtime.
    # +docs:property
    runtimeScalers: {}


  # This configuration is used when setting 'keda' to the autoscaler type.
  # NOTE: To use this feature, prometheus and KEDA must be installed in advance.
  keda:
    # Optional interval to check each trigger on.
    # For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#pollinginterval).
    # +docs:property
    # +docs:type=number
    # pollingInterval: 30

    # Optional period to wait after the last trigger reported active before scaling the resource back to 0.
    # For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#cooldownperiod).
    # +docs:property
    # +docs:type=number
    # cooldownPeriod: 300

    # Optional replica count. If this property is set, KEDA will scale the resource down to this number of replicas
    # For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#idlereplicacount).
    # +docs:property
    # +docs:type=number
    # idleReplicaCount: 0

    # Optional minimum number of replicas KEDA will scale the resource down to
    # For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#minreplicacount).
    # +docs:property
    # +docs:type=number
    # minReplicaCount: 0

    # Optional maximum number of replicas of the target resource.
    # For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#maxreplicacount).
    # +docs:type=number
    # +docs:default=100
    maxReplicaCount: 10

    # Address of Prometheus server.
    promServerAddress: http://prometheus-server.monitoring

    # Scaling trigger using Prometheus
    # For more information, see [Prometheus Scaler](https://keda.sh/docs/2.16/scalers/prometheus/#trigger-specification).
    promTriggers:
      # Query to run. You can use `{{.}}` as a placeholder for the model name.
      # e.g., avg(llmariner_active_inference_request_count{model="{{.}}"}) is converted
      # to avg(llmariner_active_inference_request_count{model="<MODEL_NAME>"})
      # NOTE: With vllm metrics alone, once scaledObject will be scaled to 0, it cannot
      # be scaled up again. Please combine with other metrics like below.
    - query: avg(llmariner_active_inference_request_count{model="{{.}}"})
      # Value to start scaling for.
      threshold: 30
      # Optional target value for activating the scaler.
      # +docs:property
      # +docs:type=number
      # activationThreshold: 0

leaderElection:
  # LeaseDuration is the duration that non-leader candidates will
  # wait to force acquire leadership. This is measured against time of
  # last observed ack. Default is 15 seconds.
  leaseDuration: ""
  # RenewDeadline is the duration that the acting controlplane will
  # retry refreshing leadership before giving up. Default is 10 seconds.
  renewDeadline: ""
  # RetryPeriod is the duration the LeaderElector clients should wait
  # between tries of actions. Default is 2 seconds.
  retryPeriod: ""

# The duration given to runnable to stop before the manager actually
# returns on stop. If not specified, manager timeouts in 30 seconds.
# +docs:property
# gracefulShutdownTimeout: ""

# Optional duration in seconds the pod needs to terminate gracefully.
# The value zero indicates stop immediately via the kill signal (no
# opportunity to shut down). If not specified, the default grace
# period (30 seconds) will be used instead.
# +docs:property
# podTerminationGracePeriodSeconds: ""

# The log level of the inference-manager-engine container.
# +docs:type=number
logLevel: 0

# Override the "inference-manager-engine.fullname" value. This value
# is used as part of most of the names of the resources created by
# this Helm chart.
# +docs:property
# fullnameOverride: "my-inference-manager-engine"

# Override the "inference-manager-engine.name" value, which is used
# to annotate some of the resources that are created by this Chart
# (using "app.kubernetes.io/name").
# +docs:property
# nameOverride: "my-inference-manager-engine"

# If enabled, a `ServiceMonitor` resource is created, which is used to
# define a scrape target for the Prometheus.
# NOTE: To use this feature, prometheus-operator must be installed in advance.
enableServiceMonitor: false

serviceAccount:
  # Specifies whether a service account should be created.
  create: true

  # The name of the service account to use. If not set and create is
  # true, a name is generated using the fullname template.
  # +docs:property
  # name: ""

# The number of replicas for the inference-manager-engine Deployment.
# +docs:type=number
replicaCount: 1

# The container image of inference-manager-engine.
image:
  # The container image name.
  repository: public.ecr.aws/cloudnatix/llmariner/inference-manager-engine
  # Kubernetes imagePullPolicy on Deployment.
  pullPolicy: IfNotPresent

# The container image of NVIDIA Triton proxy.
tritonProxyImage:
  # The container image name.
  repository: public.ecr.aws/cloudnatix/llmariner/inference-manager-triton-proxy

# Override the container image tag to deploy by setting this variable.
# If no value is set, the chart's appVersion will be used.
# +docs:property
# version: vX.Y.Z

# Optional additional annotations to add to the Deployment Pods.
# +docs:property
# podAnnotations: {}

# The nodeSelector on Pods tells Kubernetes to schedule Pods on the
# nodes with matching labels.
# For more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/).
# +docs:property
# nodeSelector: {}

# A Kubernetes Affinity, if required.
# For more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node).
#
# For example:
#   affinity:
#     nodeAffinity:
#      requiredDuringSchedulingIgnoredDuringExecution:
#        nodeSelectorTerms:
#        - matchExpressions:
#          - key: foo.bar.com/role
#            operator: In
#            values:
#            - master
#
# +docs:property
# affinity: {}

# A list of Kubernetes Tolerations, if required.
# For more information, see [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).
#
# For example:
#   tolerations:
#   - key: foo.bar.com/role
#     operator: Equal
#     value: master
#     effect: NoSchedule
#
# +docs:property
# tolerations: []

# Resources to provide to the inference-manager-engine pod.
# For more information, see [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-Containers/).
#
# For example:
#  requests:
#    cpu: 10m
#    memory: 32Mi
#
# +docs:property
resources:
  requests:
    cpu: "1000m"
    memory: "500Mi"

# readinessProbe settings for the inference-manager-engine pod.
# For more information, see [Liveness, Readiness, and Startup Probes](https://kubernetes.io/docs/concepts/configuration/liveness-readiness-startup-probes/)
readinessProbe:
  # Specify whether to enable the readiness probe.
  enabled: true
  # Number of seconds after the container has started before startup,
  # readiness or readiness probes are initiated.
  # +docs:type=number
  initialDelaySeconds: 3
  # How often (in seconds) to perform the probe. Default to 10 seconds.
  # +docs:type=number
  periodSeconds: 10
  # Number of seconds after which the probe times out.
  # +docs:type=number
  timeoutSeconds: 3
  # Minimum consecutive successes for the probe to be considered
  # successful after having failed.
  # +docs:type=number
  successThreshold: 1
  # After a probe fails `failureThreshold` times in a row, Kubernetes
  # considers that the overall check has failed: the container is not
  # ready/healthy/live.
  # +docs:type=number
  failureThreshold: 5

# Security Context for the inference-manager-engine pod.
# For more information, see [Configure a Security Context for a Pod or Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).
# +docs:property
podSecurityContext:
  fsGroup: 2000

# Security Context for the inference-manager-engine container.
# For more information, see [Configure a Security Context for a Pod or Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).
# +docs:property
securityContext:
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL
  runAsNonRoot: true
  runAsUser: 1000

# persistent volume settings for the inference-manager-engine pod.
persistentVolume:
  # If true, use a PVC. If false, use emptyDir.
  enabled: false

  # The name of the storage class for serving a persistent volume.
  storageClassName: standard

  # If defined, the loader uses the given PVC and does not create a new one.
  # NOTE: PVC must be manually created before the volume is bound.
  # +docs:property
  # existingClaim: ""

  # If defined, the loader used the PVC matched with this selectors.
  # NOTE: PVC must be manually created before the volume is bound.
  # For more information, see [Persistent Volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
  #
  # For example:
  # selector:
  #  matchLabels:
  #    release: "stable"
  #  matchExpressions:
  #    - { key: environment, operator: In, values: [ dev ] }
  #
  # +docs:property
  # selector: {}

  # If defined, the loader Deployment uses the existing PV that has been
  # provisioned in advance.
  # +docs:property
  # volumeName: ""

  # The size of volume.
  size: 100Gi

  # The access mode of the volume.
  # For more information, see [Persistent Volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)
  accessModes: ["ReadWriteOnce"]

  # If defined, the engine uses the given binding-mode for the volume.
  # +docs:property
  # volumeBindingMode: ""

# Additional volumes to add to the inference-manager-engine pod.
# For more information, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/).
# +docs:property
# volumes: []

# Additional volume mounts to add to the inference-manager-engine container.
# For more information, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/).
# +docs:property
# volumeMounts: []

# This field can be used as a condition when using it as a dependency.
# This definition is only here as a placeholder such that it is
# included in the json schema.
# +docs:hidden
# +docs:property
# enable: true
