syntax = "proto3";

package llmariner.chat.server.v1;

import "google/api/annotations.proto";

option go_package = "github.com/llmariner/inference-manager/api/v1";

// The API specification follows OpenAPI API specification (https://platform.openai.com/docs/api-reference/chat).

message CreateChatCompletionRequest {
  // Message has fields for system message, user message, assistant message, and tool message.
  message Message {
    message ToolCall {
      message Function {
        string name  = 1;
        string arguments = 2;
      }
      string id = 1;
      string type = 2;
      Function function = 3;
    }

    message Content {
      // Refer to https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages
      message ImageUrl {
        string url = 1;
        string detail = 2;
      }
      message InputAudio {
        string data = 1;
        string format = 2;
      }
      // AudioUrl is non-openai compatible. It is used by vLLM to handle audio input.
      message AudioUrl {
        string url = 1;
      }

      string type = 1;

      string text = 2;
      ImageUrl image_url = 3;
      InputAudio input_audio = 4;
      AudioUrl audio_url = 5;
    }
    // The type of the content is either string or array in the OpenAI API spec, but
    // we only support the array type here. When a client sends a string, it is converted before
    // the request is being unmarshalled.
    repeated Content content = 5;
    string role = 2;
    string name = 3;
    repeated ToolCall tool_calls = 4;

    // Used by the "Tool message".
    string tool_call_id = 6;

    // Next ID: 7
  }

  message ToolChoice {
    string type = 2;
    message Function {
      string name = 1;
    }
    Function function = 3;
  }

  message ResponseFormat {
    string type = 1;
  }

  message Tool {
    message Function {
      string description = 1;
      string name = 2;
      // OpenAI API has the "parameters" field that stores a JSON Schema object.
      // As we cannot have a JSON Schema object in the proto, we encode the JSON Schema object
      // with base64 and store it before unmarshalling it in the server.
      //
      // The field is converted to the "parameters" field when the request is being
      // sent to an inference runtime.
      string encoded_parameters = 3;
    }
    string type = 1;
    Function function = 2;
  }

  message StreamOptions {
    bool include_usage = 1;
  }

  // TODO(kenji): Revisit this.
  repeated Message messages = 1;
  string model = 2;
  double frequency_penalty = 3;
  map<string, double> logit_bias = 4;
  bool logprobs = 5;
  int32 top_logprobs = 6;
  int32 max_tokens = 7;
  int32 n = 8;
  double presence_penalty = 9;
  ResponseFormat response_format = 10;
  int32 seed = 11;
  // string / arrary/ null
  repeated string stop = 12;
  bool stream = 13;
  StreamOptions stream_options = 19;
  double temperature = 14;
  double top_p = 15;
  repeated Tool tools = 16;
  // The type of the "tool_choice" field is a string or an object.
  // As we cannot have a string or an object in the proto, we use the "tool_choice_obj"
  // if a request sets the "tool_choice" field as an object.
  //
  // The "tool_choice_object" field is converted to the "tool_choice" field when the request is being
  // sent to an inference runtime.
  string tool_choice = 17;
  ToolChoice tool_choice_object = 20;
  string user = 18;

  // Next ID: 21
}

message ToolCall {
  message Function {
    string name  = 1;
    string arguments = 2;
  }
  string id = 1;
  string type = 2;
  Function function = 3;
}

message Logprobs {
  message Content {
    message TopLogprobs {
      string token = 1;
      double logprob = 2;
      bytes bytes = 3;
    }

    string token = 1;
    double logprob = 2;
    // A list of integers representing the UTF-8 bytes representation of the token.
    bytes bytes = 3;
    TopLogprobs top_logprobs = 4;
  }
  repeated Content content = 1;
}

message Usage {
  int32 completion_tokens = 1;
  int32 prompt_tokens = 2;
  int32 total_tokens = 3;
}

message ChatCompletion {
  message Choice {
    message Message {

      string content = 1;
      repeated ToolCall tool_calls = 2;
      string role = 3;
    }

    string finish_reason = 1;
    int32 index = 2;
    Message message = 3;
    Logprobs logprobs = 4;
  }

  string id = 1;
  repeated Choice choices = 2;
  int32 created = 3;
  string model = 4;
  string system_fingerprint = 5;
  string object = 6;
  Usage usage = 7;
}

message ChatCompletionChunk {
  message Choice {
    message Delta {
      message ToolCall {
	message Function {
	  string name  = 1;
	  string arguments = 2;
	}
	string id = 1;
	string type = 2;
	Function function = 3;
      }

      string content = 1;
      repeated ToolCall tool_calls = 2;
      string role = 3;
    }

    Delta delta = 1;
    string finish_reason = 2;
    int32 index = 3;
    Logprobs logprobs = 4;
  }

  string id = 1;
  repeated Choice choices = 2;
  int32 created = 3;
  string model = 4;
  string system_fingerprint = 5;
  string object = 6;
  Usage usage = 7;
}

// RagFunction is used to unmarshal the json string specified in `Parameters` of Tool message.
message RagFunction {
  string vector_store_name = 1;
}

service ChatService {
  // CreateChatCompletion is implemented as an HTTP handler without gRPC
  // as gRPC gateway does not support server-sent events.
}


// Define legacy completion
// (https://platform.openai.com/docs/api-reference/completions/create).

message CreateCompletionRequest {
  message StreamOption {
    bool include_usage = 1;
  }
  string model = 1;
  // This can be a string or an array of strings, but we use string assuming that it is more common.
  string prompt = 2;
  int32 best_of = 3;
  bool echo = 4;
  double frequency_penalty = 5;
  map<string, double> logit_bias = 6;
  int32 logprobs = 7;
  int32 max_tokens = 8;
  int32 n = 9;
  double presence_penalty = 10;
  int32 seed = 11;
  repeated string stop = 12;
  bool stream = 13;
  StreamOption stream_option = 14;
  string suffix = 15;
  double temperature = 16;
  double top_p = 17;
  string user = 18;
}

message Completion {
  message Choice {
    message Logprobs {
      // TODO: Revisit this. The types of the fields are not clearly specified in the spec.
      int32 text_offset = 1;
      double token_logprobs = 2;
      string tokens = 3;
      double top_logprobs = 4;
    }
    string finish_reason = 1;
    int32 index = 2;
    Logprobs logprobs = 3;
    string text = 4;
  }

  string id = 1;
  repeated Choice choices = 2;
  int32 created = 3;
  string model = 4;
  string system_fingerprint = 5;
  string object = 6;

  Usage usage = 7;
}
