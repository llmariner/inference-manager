syntax = "proto3";

package llmariner.inference.server.v1;

import "google/api/annotations.proto";

option go_package = "github.com/llmariner/inference-manager/api/v1";

message EngineStatus {
  message Model {
    message StatusDetails {
      int32 num_ready_pods = 1;
      int32 num_total_pods = 2;
      string status_message = 3;
    }

    string id = 1;
    bool is_ready = 2;
    int32 in_progress_task_count = 3;
    int32 gpu_allocated = 4;

    // is_dynamically_loaded_lora indicates whether the model is a LoRA
    // that is dynamically loaded onto its base model.
    bool is_dynamically_loaded_lora = 5;

    StatusDetails status_details = 6;
  }

  string engine_id = 1;
  bool ready = 4;
  repeated Model models = 5;
  string cluster_id = 6;
}

message InferenceStatus {
  repeated ClusterStatus cluster_statuses = 1;
  TaskStatus task_status = 2;
}

message TaskStatus {
  // in_progress_task_counts tracks the number of in-progress tasks grouped by model id.
  map<string, int32> in_progress_task_counts = 1;
}

message ClusterStatus {
  string id = 1;
  string name = 2;
  // TODO(kenji): Revisit. Each engine in the same cluster reports the same information on models.
  // It might be better to just report the model information.
  repeated EngineStatus engine_statuses = 3;

  int32 model_count = 4;
  int32 in_progress_task_count = 5;
  int32 gpu_allocated = 6;
}

message GetInferenceStatusRequest {}

service InferenceService {
  rpc GetInferenceStatus(GetInferenceStatusRequest) returns (InferenceStatus) {
    option (google.api.http) = {
      get: "/v1/inference/status"
    };
  }

}
