global:
  objectStore:
    s3:
      endpointUrl:
      region:
      bucket:
      assumeRole:
        roleArn:
        externalId:

  awsSecret:
    name:
    accessKeyIdKey:
    secretAccessKeyKey:

  worker:
    registrationKeySecret:
      name:
      key:
    tls:
      enable: false
    # If specified, use this as the address for accessing the control-plane services.
    controlPlaneAddr: ""

  controlPlaneAddr: ""

runtime:
  runtimeImages:
    ollama: mirror.gcr.io/ollama/ollama:0.3.6
    vllm: mirror.gcr.io/vllm/vllm-openai:v0.6.3
    # Release 2.50.0 (https://github.com/triton-inference-server/server/releases/tag/v2.50.0).
    # We might need to recompile models when we upgrade the version of Triton Inference Server
    # and TensortRT-LLM.
    triton: nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3
  imagePullPolicy: IfNotPresent

model:
  # If you want to control pod and volume availability zone and topologies,
  # please use the `WaitForFirstConsumer` as the `volumeBindingMode` and
  # ensure that your CSI plugin supports this mode.
  # https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
  default:
    runtimeName: ollama
    resources:
      requests:
        cpu: "1000m"
        memory: "500Mi"
      limits:
        nvidia.com/gpu: 1
      # volume:
      #   storageClassName: "fast"
      #   size: "100Gi"
      #   accessMode: "ReadWriteOnce"
    replicas: 1
    preloaded: false
    contextLength: 0
    vllmExtraFlags:

  overrides:
    # sample-model:
    #   runtimeName: ollama
    #   resources:
    #     requests:
    #       cpu: "100m"
    #       memory: "128Mi"
    #     limits:
    #       cpu: "200m"
    #       memory: "256Mi"
    #     volume:
    #       storageClassName: "standard"
    #       size: "1Gi"
    #       accessMode: "ReadWriteOnce"
    #   replicas: 1
    #   preloaded: true
    #   contextLength: 1024

ollama:
  # Keep models in memory for a long period of time as we don't want end users to
  # hit slowness due to GPU memory loading.
  keepAlive: 96h
  # If set, Ollama attemts to all GPUs in a node.
  # Setting this to true was mentioned in https://github.com/ollama/ollama/issues/5494
  # to support multiple H100s, but this setting didn't help.
  forceSpreading: false
  debug: false
  runnersDir: /tmp/ollama-runners

autoscaler:
  enable: false
  initialDelay: "12s"
  syncPeriod: "2s"
  scaleToZeroGracePeriod: "5m"
  metricsWindow: "60s"
  defaultScaler:
    targetValue: 100
    maxReplicas: 10
    minReplicas: 1
    maxScaleUpRate: 3.0
    maxScaleDownRate: 0.5

# maxConcurentRequests is the maximun number of requests procssed in parallel.
maxConcurrentRequests: 0

healthPort: 8081

leaderElection:
  leaseDuration:
  renewDeadline:
  retryPeriod:

preloadedModelIds:

modelContextLengths:

# The following default values work if model-manager-server runs in the same namespace.
inferenceManagerServerWorkerServiceAddr: inference-manager-server-worker-service-grpc:8082
modelManagerServerWorkerServiceAddr: model-manager-server-worker-service-grpc:8082

gracefulShutdownTimeout:
podTerminationGracePeriodSeconds:

logLevel: 0

replicaCount: 1
image:
  repository: public.ecr.aws/cloudnatix/llmariner/inference-manager-engine
  pullPolicy: IfNotPresent

tritonProxyImage:
  repository: public.ecr.aws/cloudnatix/llmariner/inference-manager-triton-proxy

serviceAccount:
  # Set this to false if an existing service account is used.
  create: true
  name: ""

podAnnotations:
nodeSelector:
affinity:
tolerations:

version:

resources:
  requests:
    cpu: "1000m"
    memory: "500Mi"

podSecurityContext:
  fsGroup: 2000
securityContext:
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL
  runAsNonRoot: true
  runAsUser: 1000

persistentVolume:
  # If true, engine will create/use a PVC. If false, use emptyDir
  enabled: false
  # If defined, the engine uses the given PVC and does not create a new one.
  # NOTE: PVC must be manually created before the volume is bound.
  existingClaim:

  storageClassName:
  # If defined, the engine uses the existing PV that has been provisioned in advance.
  volumeName:

  size: 100Gi
  accessModes:
  - ReadWriteOnce

  volumeBindingMode:
  selector:

readinessProbe:
  httpGet:
    path: /ready
    port: health
    scheme: HTTP
  initialDelaySeconds: 3
  failureThreshold: 5
  timeoutSeconds: 3

enableServiceMonitor: false
