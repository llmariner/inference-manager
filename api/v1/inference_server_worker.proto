syntax = "proto3";

package llmariner.inference.server.v1;

import "api/v1/inference_server_audio.proto";
import "api/v1/inference_server_chat.proto";
import "api/v1/inference_server_embeddings.proto";
import "api/v1/inference_server_management.proto";
import "api/v1/inference_server_model_response.proto";
import "api/v1/inference_server_tokenize.proto";

option go_package = "github.com/llmariner/inference-manager/api/v1";

message HeaderValue {
  repeated string values = 1;
}

message HttpResponse {
  int32 status_code = 1;
  string status = 2;
  map<string, HeaderValue> header = 3;
  // body is empty for server sent events.
  bytes body = 4;

  // latency_ms represents the latency in milliseconds
  // between inference-manager-engine and an inference-runtime.
  int32 latency_ms = 5;
}

message ServerSentEvent {
  bytes data = 1;
  bool is_last_event = 2;

  // latency_ms represents the latency in milliseconds
  // between inference-manager-engine and an inference-runtime.
  // This is set when is_last_event is true.
  int32 latency_ms = 3;
}

message TaskResult {
  string task_id = 1;
  oneof message {
    HttpResponse http_response = 2;
    ServerSentEvent server_sent_event = 3;
  }

  // result_index is used to identify the index of the result and make sure results are processed in order.
  int32 result_index = 4;
}

message ProcessTasksRequest {
  oneof message {
    EngineStatus engine_status = 1;
    TaskResult task_result = 2;
  }
}

message GoAwayRequest {
}

message HeartbeatRequest {
}

message TaskRequest {
  oneof request {
    llmariner.chat.server.v1.CreateChatCompletionRequest chat_completion = 1;
    llmariner.embeddings.server.v1.CreateEmbeddingRequest embedding = 2;
    llmariner.audio.server.v1.CreateAudioTranscriptionRequest audio_transcription = 7;
    llmariner.response.server.v1.CreateModelResponseRequest model_response = 8;
    llmariner.tokenize.server.v1.TokenizeRequest tokenize_request = 9;

    // go_away is used to signal the worker to disconnect and
    // reconnect to other server.
    GoAwayRequest go_away = 5;
    HeartbeatRequest heartbeat = 6;

    // Next ID: 8
  }
}

message Task {
  string id = 1;

  TaskRequest request = 4;
  map<string, HeaderValue> header = 3;

  string engine_id = 5;

  int32 timeout_seconds = 6;

  // Next ID: 7
}

message ProcessTasksResponse {
  Task new_task = 1;
}

service InferenceWorkerService {
  rpc ProcessTasks(stream ProcessTasksRequest) returns (stream ProcessTasksResponse) {}
}
