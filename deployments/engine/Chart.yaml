apiVersion: v2
name: inference-manager-engine
description: The inference-manager-engine manages inference runtimes (e.g., vLLM and Ollama) in containers, load models, and process requests. Also, auto-scale runtimes based on the number of in-flight requests.
home: https://llmariner.ai/
icon: https://raw.githubusercontent.com/llmariner/llmariner.github.io/refs/heads/main/assets/icons/logo.svg
type: application
version: 1.35.1
appVersion: 1.35.1
sources:
  - https://github.com/llmariner/inference-manager
keywords:
  - llmariner
annotations:
  "artifacthub.io/category": ai-machine-learning
  "artifacthub.io/license": Apache-2.0
