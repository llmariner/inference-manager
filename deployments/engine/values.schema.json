{"$schema":"http://json-schema.org/draft-07/schema#","$ref":"#/$defs/helm-values","$defs":{"helm-values":{"type":"object","properties":{"affinity":{"$ref":"#/$defs/helm-values.affinity"},"autoscaler":{"$ref":"#/$defs/helm-values.autoscaler"},"componentStatusSender":{"$ref":"#/$defs/helm-values.componentStatusSender"},"enable":{"$ref":"#/$defs/helm-values.enable"},"enableServiceMonitor":{"$ref":"#/$defs/helm-values.enableServiceMonitor"},"fullnameOverride":{"$ref":"#/$defs/helm-values.fullnameOverride"},"global":{"$ref":"#/$defs/helm-values.global"},"gracefulShutdownTimeout":{"$ref":"#/$defs/helm-values.gracefulShutdownTimeout"},"healthPort":{"$ref":"#/$defs/helm-values.healthPort"},"image":{"$ref":"#/$defs/helm-values.image"},"inferenceManagerServerWorkerServiceAddr":{"$ref":"#/$defs/helm-values.inferenceManagerServerWorkerServiceAddr"},"leaderElection":{"$ref":"#/$defs/helm-values.leaderElection"},"logLevel":{"$ref":"#/$defs/helm-values.logLevel"},"metricsPort":{"$ref":"#/$defs/helm-values.metricsPort"},"model":{"$ref":"#/$defs/helm-values.model"},"modelContextLengths":{"$ref":"#/$defs/helm-values.modelContextLengths"},"modelManagerServerWorkerServiceAddr":{"$ref":"#/$defs/helm-values.modelManagerServerWorkerServiceAddr"},"nameOverride":{"$ref":"#/$defs/helm-values.nameOverride"},"nim":{"$ref":"#/$defs/helm-values.nim"},"nodeSelector":{"$ref":"#/$defs/helm-values.nodeSelector"},"ollama":{"$ref":"#/$defs/helm-values.ollama"},"persistentVolume":{"$ref":"#/$defs/helm-values.persistentVolume"},"podAnnotations":{"$ref":"#/$defs/helm-values.podAnnotations"},"podSecurityContext":{"$ref":"#/$defs/helm-values.podSecurityContext"},"preloadedModelIds":{"$ref":"#/$defs/helm-values.preloadedModelIds"},"readinessProbe":{"$ref":"#/$defs/helm-values.readinessProbe"},"replicaCount":{"$ref":"#/$defs/helm-values.replicaCount"},"resources":{"$ref":"#/$defs/helm-values.resources"},"runtime":{"$ref":"#/$defs/helm-values.runtime"},"securityContext":{"$ref":"#/$defs/helm-values.securityContext"},"serviceAccount":{"$ref":"#/$defs/helm-values.serviceAccount"},"terminationGracePeriodSeconds":{"$ref":"#/$defs/helm-values.terminationGracePeriodSeconds"},"tolerations":{"$ref":"#/$defs/helm-values.tolerations"},"tritonProxyImage":{"$ref":"#/$defs/helm-values.tritonProxyImage"},"version":{"$ref":"#/$defs/helm-values.version"},"vllm":{"$ref":"#/$defs/helm-values.vllm"},"volumeMounts":{"$ref":"#/$defs/helm-values.volumeMounts"},"volumes":{"$ref":"#/$defs/helm-values.volumes"}},"additionalProperties":false},"helm-values.affinity":{"description":"A Kubernetes Affinity, if required.\nFor more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node).\n\nFor example:\naffinity:\n  nodeAffinity:\n   requiredDuringSchedulingIgnoredDuringExecution:\n     nodeSelectorTerms:\n     - matchExpressions:\n       - key: foo.bar.com/role\n         operator: In\n         values:\n         - master","type":"object"},"helm-values.autoscaler":{"type":"object","properties":{"builtin":{"$ref":"#/$defs/helm-values.autoscaler.builtin"},"enable":{"$ref":"#/$defs/helm-values.autoscaler.enable"},"keda":{"$ref":"#/$defs/helm-values.autoscaler.keda"},"type":{"$ref":"#/$defs/helm-values.autoscaler.type"}},"additionalProperties":false},"helm-values.autoscaler.builtin":{"type":"object","properties":{"defaultScaler":{"$ref":"#/$defs/helm-values.autoscaler.builtin.defaultScaler"},"initialDelay":{"$ref":"#/$defs/helm-values.autoscaler.builtin.initialDelay"},"metricsWindow":{"$ref":"#/$defs/helm-values.autoscaler.builtin.metricsWindow"},"runtimeScalers":{"$ref":"#/$defs/helm-values.autoscaler.builtin.runtimeScalers"},"scaleToZeroGracePeriod":{"$ref":"#/$defs/helm-values.autoscaler.builtin.scaleToZeroGracePeriod"},"syncPeriod":{"$ref":"#/$defs/helm-values.autoscaler.builtin.syncPeriod"}},"additionalProperties":false},"helm-values.autoscaler.builtin.defaultScaler":{"type":"object","properties":{"maxReplicas":{"$ref":"#/$defs/helm-values.autoscaler.builtin.defaultScaler.maxReplicas"},"maxScaleDownRate":{"$ref":"#/$defs/helm-values.autoscaler.builtin.defaultScaler.maxScaleDownRate"},"maxScaleUpRate":{"$ref":"#/$defs/helm-values.autoscaler.builtin.defaultScaler.maxScaleUpRate"},"minReplicas":{"$ref":"#/$defs/helm-values.autoscaler.builtin.defaultScaler.minReplicas"},"targetValue":{"$ref":"#/$defs/helm-values.autoscaler.builtin.defaultScaler.targetValue"}},"additionalProperties":false},"helm-values.autoscaler.builtin.defaultScaler.maxReplicas":{"description":"the maximum number of replicas. e.g., if this is 10, the pod can be scaled up to 10.","type":"number","default":10},"helm-values.autoscaler.builtin.defaultScaler.maxScaleDownRate":{"description":"The maximum rate of scaling down.\ne.g., current replicas is 6 and this rate is 0.5, the pod can be scaled down to 3. (floor(6 * 0.5) = 3)","type":"number","default":0.5},"helm-values.autoscaler.builtin.defaultScaler.maxScaleUpRate":{"description":"The maximum rate of scaling up.\ne.g., current replicas is 2 and this rate is 3.0, the pod can be scaled up to 6. (ceil(2 * 3.0) = 6)","type":"number","default":3},"helm-values.autoscaler.builtin.defaultScaler.minReplicas":{"description":"the minimum number of replicas. e.g., if this is 0, the pod can be scaled down to 0.","type":"number","default":1},"helm-values.autoscaler.builtin.defaultScaler.targetValue":{"description":"The per-pod metric value that we target to maintain. Currently, this is the concurrent requests per model runtime.","type":"number","default":100},"helm-values.autoscaler.builtin.initialDelay":{"description":"The initial delay before starting the autoscaler.","type":"string","default":"12s"},"helm-values.autoscaler.builtin.metricsWindow":{"description":"the window size for metrics. e.g., if it's 5 minutes, we'll use the 5-minute average as the metric.","type":"string","default":"60s"},"helm-values.autoscaler.builtin.runtimeScalers":{"description":"Optional a map of runtime name to scaler settings. It overrides the default scaling settings for the runtime.","type":"object","default":{}},"helm-values.autoscaler.builtin.scaleToZeroGracePeriod":{"description":"The grace period before scaling to zero.","type":"string","default":"5m"},"helm-values.autoscaler.builtin.syncPeriod":{"description":"The period for calculating the scaling.","type":"string","default":"2s"},"helm-values.autoscaler.enable":{"description":"If set to true, the request base autoscaler will be enabled. NOTE: In ollama dynamic-model-loading mode, volume sharing is required.","type":"boolean","default":false},"helm-values.autoscaler.keda":{"type":"object","properties":{"cooldownPeriod":{"$ref":"#/$defs/helm-values.autoscaler.keda.cooldownPeriod"},"idleReplicaCount":{"$ref":"#/$defs/helm-values.autoscaler.keda.idleReplicaCount"},"maxReplicaCount":{"$ref":"#/$defs/helm-values.autoscaler.keda.maxReplicaCount"},"minReplicaCount":{"$ref":"#/$defs/helm-values.autoscaler.keda.minReplicaCount"},"pollingInterval":{"$ref":"#/$defs/helm-values.autoscaler.keda.pollingInterval"},"promServerAddress":{"$ref":"#/$defs/helm-values.autoscaler.keda.promServerAddress"},"promTriggers":{"$ref":"#/$defs/helm-values.autoscaler.keda.promTriggers"}},"additionalProperties":false},"helm-values.autoscaler.keda.cooldownPeriod":{"description":"Optional period to wait after the last trigger reported active before scaling the resource back to 0. For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#cooldownperiod).","type":"number"},"helm-values.autoscaler.keda.idleReplicaCount":{"description":"Optional replica count. If this property is set, KEDA will scale the resource down to this number of replicas. For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#idlereplicacount).","type":"number"},"helm-values.autoscaler.keda.maxReplicaCount":{"description":"Optional maximum number of replicas of the target resource. For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#maxreplicacount).","type":"number","default":100},"helm-values.autoscaler.keda.minReplicaCount":{"description":"Optional minimum number of replicas KEDA will scale the resource down to. For more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#minreplicacount).","type":"number"},"helm-values.autoscaler.keda.pollingInterval":{"description":"Optional interval to check each trigger on.\nFor more information, see [ScalingObject Spec](https://keda.sh/docs/2.16/reference/scaledobject-spec/#pollinginterval).","type":"number"},"helm-values.autoscaler.keda.promServerAddress":{"description":"Address of Prometheus server.","type":"string","default":"http://prometheus-server.monitoring"},"helm-values.autoscaler.keda.promTriggers":{"type":"array","items":{"$ref":"#/$defs/helm-values.autoscaler.keda.promTriggers[0]"}},"helm-values.autoscaler.keda.promTriggers[0]":{"type":"object","properties":{"activationThreshold":{"$ref":"#/$defs/helm-values.autoscaler.keda.promTriggers[0].activationThreshold"},"query":{"$ref":"#/$defs/helm-values.autoscaler.keda.promTriggers[0].query"},"threshold":{"$ref":"#/$defs/helm-values.autoscaler.keda.promTriggers[0].threshold"}},"additionalProperties":false},"helm-values.autoscaler.keda.promTriggers[0].activationThreshold":{"description":"Optional target value for activating the scaler.","type":"number"},"helm-values.autoscaler.keda.promTriggers[0].query":{"type":"string","default":"avg(llmariner_active_inference_request_count{model=\"{{.}}\"})"},"helm-values.autoscaler.keda.promTriggers[0].threshold":{"description":"Value to start scaling for.","type":"number","default":30},"helm-values.autoscaler.type":{"description":"The type of autoscaler.","type":"string","default":"builtin"},"helm-values.componentStatusSender":{"type":"object","properties":{"clusterManagerServerWorkerServiceAddr":{"$ref":"#/$defs/helm-values.componentStatusSender.clusterManagerServerWorkerServiceAddr"},"enable":{"$ref":"#/$defs/helm-values.componentStatusSender.enable"},"initialDelay":{"$ref":"#/$defs/helm-values.componentStatusSender.initialDelay"},"interval":{"$ref":"#/$defs/helm-values.componentStatusSender.interval"},"name":{"$ref":"#/$defs/helm-values.componentStatusSender.name"}},"additionalProperties":false},"helm-values.componentStatusSender.clusterManagerServerWorkerServiceAddr":{"description":"The address of the cluster-manager-server to call worker services.","type":"string","default":"cluster-manager-server-worker-service-grpc:8082"},"helm-values.componentStatusSender.enable":{"description":"The flag to enable sending component status to the cluster-manager-server.","type":"boolean","default":true},"helm-values.componentStatusSender.initialDelay":{"description":"initialDelay is the time to wait before starting the sender.","type":"string","default":"1m"},"helm-values.componentStatusSender.interval":{"description":"The interval time to send the component status.","type":"string","default":"15m"},"helm-values.componentStatusSender.name":{"description":"The name of the component.","type":"string","default":"inference-manager-engine"},"helm-values.enable":{"description":"This field can be used as a condition when using it as a dependency. This definition is only here as a placeholder such that it is included in the json schema.","type":"boolean"},"helm-values.enableServiceMonitor":{"description":"If enabled, a `ServiceMonitor` resource is created, which is used to define a scrape target for the Prometheus. NOTE: To use this feature, prometheus-operator must be installed in advance.","type":"boolean","default":false},"helm-values.fullnameOverride":{"description":"Override the \"inference-manager-engine.fullname\" value. This value is used as part of most of the names of the resources created by this Helm chart.","type":"string"},"helm-values.global":{"description":"Global values shared across all (sub)charts","type":"object","properties":{"awsSecret":{"$ref":"#/$defs/helm-values.global.awsSecret"},"objectStore":{"$ref":"#/$defs/helm-values.global.objectStore"},"worker":{"$ref":"#/$defs/helm-values.global.worker"}}},"helm-values.global.awsSecret":{"type":"object","properties":{"accessKeyIdKey":{"$ref":"#/$defs/helm-values.global.awsSecret.accessKeyIdKey"},"name":{"$ref":"#/$defs/helm-values.global.awsSecret.name"},"secretAccessKeyKey":{"$ref":"#/$defs/helm-values.global.awsSecret.secretAccessKeyKey"}}},"helm-values.global.awsSecret.accessKeyIdKey":{"description":"The key name with an access key ID set.","type":"string","default":"accessKeyId"},"helm-values.global.awsSecret.name":{"description":"The secret name.","type":"string"},"helm-values.global.awsSecret.secretAccessKeyKey":{"description":"The key name with a secret access key set.","type":"string","default":"secretAccessKey"},"helm-values.global.objectStore":{"type":"object","properties":{"s3":{"$ref":"#/$defs/helm-values.global.objectStore.s3"}}},"helm-values.global.objectStore.s3":{"type":"object","properties":{"assumeRole":{"$ref":"#/$defs/helm-values.global.objectStore.s3.assumeRole"},"bucket":{"$ref":"#/$defs/helm-values.global.objectStore.s3.bucket"},"endpointUrl":{"$ref":"#/$defs/helm-values.global.objectStore.s3.endpointUrl"},"region":{"$ref":"#/$defs/helm-values.global.objectStore.s3.region"}}},"helm-values.global.objectStore.s3.assumeRole":{"description":"Optional AssumeRole.\nFor more information, see [AssumeRole](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html).","type":"object"},"helm-values.global.objectStore.s3.bucket":{"description":"The bucket name to store data.","type":"string","default":"llmariner"},"helm-values.global.objectStore.s3.endpointUrl":{"description":"Optional endpoint URL for the object store.","type":"string"},"helm-values.global.objectStore.s3.region":{"description":"The region name.","type":"string","default":"dummy"},"helm-values.global.worker":{"type":"object","properties":{"controlPlaneAddr":{"$ref":"#/$defs/helm-values.global.worker.controlPlaneAddr"},"registrationKeySecret":{"$ref":"#/$defs/helm-values.global.worker.registrationKeySecret"},"tls":{"$ref":"#/$defs/helm-values.global.worker.tls"}}},"helm-values.global.worker.controlPlaneAddr":{"description":"If specified, use this address for accessing the control-plane. This is necessary when installing LLMariner in a multi-cluster mode. For more information, see [Install across Multiple Clusters](https://llmariner.ai/docs/setup/install/multi_cluster_production/).","type":"string","default":""},"helm-values.global.worker.registrationKeySecret":{"type":"object","properties":{"key":{"$ref":"#/$defs/helm-values.global.worker.registrationKeySecret.key"},"name":{"$ref":"#/$defs/helm-values.global.worker.registrationKeySecret.name"}}},"helm-values.global.worker.registrationKeySecret.key":{"description":"The key name with a registration key set.","type":"string","default":"key"},"helm-values.global.worker.registrationKeySecret.name":{"description":"The secret name. `default-cluster-registration-key` is available when the control-plane and worker-plane are in the same cluster. This Secret is generated by cluster-manager-server as default. For more information, see [Install across Multiple Clusters](https://llmariner.ai/docs/setup/install/multi_cluster_production/).","type":"string","default":"default-cluster-registration-key"},"helm-values.global.worker.tls":{"type":"object","properties":{"enable":{"$ref":"#/$defs/helm-values.global.worker.tls.enable"}}},"helm-values.global.worker.tls.enable":{"description":"The flag to enable TLS access to the control-plane.","type":"boolean","default":false},"helm-values.gracefulShutdownTimeout":{"description":"The duration given to runnable to stop before the manager actually returns on stop. If not specified, manager timeouts in 30 seconds.","type":"string"},"helm-values.healthPort":{"description":"The HTTP port number for the health check.","type":"number","default":8081},"helm-values.image":{"type":"object","properties":{"pullPolicy":{"$ref":"#/$defs/helm-values.image.pullPolicy"},"repository":{"$ref":"#/$defs/helm-values.image.repository"}},"additionalProperties":false},"helm-values.image.pullPolicy":{"description":"Kubernetes imagePullPolicy on Deployment.","type":"string","default":"IfNotPresent"},"helm-values.image.repository":{"description":"The container image name.","type":"string","default":"public.ecr.aws/cloudnatix/llmariner/inference-manager-engine"},"helm-values.inferenceManagerServerWorkerServiceAddr":{"description":"The address of the inference-manager-server to call inference APIs for workers.","type":"string","default":"inference-manager-server-worker-service-grpc:8082"},"helm-values.leaderElection":{"type":"object","properties":{"leaseDuration":{"$ref":"#/$defs/helm-values.leaderElection.leaseDuration"},"renewDeadline":{"$ref":"#/$defs/helm-values.leaderElection.renewDeadline"},"retryPeriod":{"$ref":"#/$defs/helm-values.leaderElection.retryPeriod"}},"additionalProperties":false},"helm-values.leaderElection.leaseDuration":{"description":"LeaseDuration is the duration that non-leader candidates will wait to force acquire leadership. This is measured against time of last observed ack. Default is 15 seconds.","type":"string","default":""},"helm-values.leaderElection.renewDeadline":{"description":"RenewDeadline is the duration that the acting controlplane will retry refreshing leadership before giving up. Default is 10 seconds.","type":"string","default":""},"helm-values.leaderElection.retryPeriod":{"description":"RetryPeriod is the duration the LeaderElector clients should wait between tries of actions. Default is 2 seconds.","type":"string","default":""},"helm-values.logLevel":{"description":"The log level of the inference-manager-engine container.","type":"number","default":0},"helm-values.metricsPort":{"description":"The HTTP port number for the metrics check.","type":"number","default":8084},"helm-values.model":{"type":"object","properties":{"default":{"$ref":"#/$defs/helm-values.model.default"},"overrides":{"$ref":"#/$defs/helm-values.model.overrides"}},"additionalProperties":false},"helm-values.model.default":{"type":"object","properties":{"containerRuntimeClassName":{"$ref":"#/$defs/helm-values.model.default.containerRuntimeClassName"},"contextLength":{"$ref":"#/$defs/helm-values.model.default.contextLength"},"preloaded":{"$ref":"#/$defs/helm-values.model.default.preloaded"},"replicas":{"$ref":"#/$defs/helm-values.model.default.replicas"},"resources":{"$ref":"#/$defs/helm-values.model.default.resources"},"runtimeName":{"$ref":"#/$defs/helm-values.model.default.runtimeName"},"schedulerName":{"$ref":"#/$defs/helm-values.model.default.schedulerName"},"vllmExtraFlags":{"$ref":"#/$defs/helm-values.model.default.vllmExtraFlags"}},"additionalProperties":false},"helm-values.model.default.containerRuntimeClassName":{"description":"The name of a K8s Runtime Class (https://kubernetes.io/docs/concepts/containers/runtime-class/) used by model runtime. This is set the Runtime Class of Nvidia container runtime if it is not a cluster default.","type":"string","default":""},"helm-values.model.default.contextLength":{"description":"Specify the context length. If set 0, the default context length is used.","type":"number","default":0},"helm-values.model.default.preloaded":{"type":"boolean","default":false},"helm-values.model.default.replicas":{"description":"The number of replicas for the runtime StatefulSet. NOTE: In ollama dynamic-model-loading mode, volume sharing is required when creating one or more replicas.","type":"number","default":1},"helm-values.model.default.resources":{"type":"object","properties":{"limits":{"$ref":"#/$defs/helm-values.model.default.resources.limits"},"requests":{"$ref":"#/$defs/helm-values.model.default.resources.requests"},"volume":{"$ref":"#/$defs/helm-values.model.default.resources.volume"}},"additionalProperties":false},"helm-values.model.default.resources.limits":{"description":"The resource limits for the runtime pod.","type":"object","default":{"nvidia.com/gpu":1}},"helm-values.model.default.resources.requests":{"description":"The resource requests for the runtime pod.","type":"object","default":{"cpu":"1000m","memory":"500Mi"}},"helm-values.model.default.resources.volume":{"description":"The persistent volume settings for the runtime. If not specified, the runtime StatefulSet use an emptyDir instead. If `shareWithReplicas` is enabled, all replicas in the StatefulSet share the same persistent volume. Otherwise, a persistent volume is created for each replica.\n\nFor example:\nvolume:\n  shareWithReplicas: false\n  storageClassName: \"fast\"\n  size: \"100Gi\"\n  accessMode: \"ReadWriteOnce\"","type":"object","default":{}},"helm-values.model.default.runtimeName":{"description":"The runtime name to use for inference.","type":"string","default":"ollama"},"helm-values.model.default.schedulerName":{"description":"The name of a K8s scheduler used by model runtime.","type":"string","default":""},"helm-values.model.default.vllmExtraFlags":{"description":"Optional a list of vLLM extra flags.","type":"array","items":{}},"helm-values.model.overrides":{"description":"Optional a map of model ID to model settings. It overrides the default settings for the model.\n\nFor example:\nsample-model:\n  runtimeName: ollama\n  resources:\n    requests:\n      cpu: \"100m\"\n      memory: \"128Mi\"\n    limits:\n      cpu: \"200m\"\n      memory: \"256Mi\"\n    volume:\n      storageClassName: \"standard\"\n      size: \"1Gi\"\n      accessMode: \"ReadWriteOnce\"\n  replicas: 1\n  preloaded: true\n  contextLength: 1024","type":"object","default":{}},"helm-values.modelContextLengths":{"description":"Optional a map of model ID to context length. If not specified, the default context length is used.\n\nFor example:\nmodelContextLengths:\n  google/gemma-2b-it-q4_0: 1024","type":"object"},"helm-values.modelManagerServerWorkerServiceAddr":{"description":"The address of the model-manager-server to call model APIs for workers.","type":"string","default":"model-manager-server-worker-service-grpc:8082"},"helm-values.nameOverride":{"description":"Override the \"inference-manager-engine.name\" value, which is used to annotate some of the resources that are created by this Chart\n(using \"app.kubernetes.io/name\").","type":"string"},"helm-values.nim":{"type":"object","properties":{"models":{"$ref":"#/$defs/helm-values.nim.models"},"ngcApiKey":{"$ref":"#/$defs/helm-values.nim.ngcApiKey"}},"additionalProperties":false},"helm-values.nim.models":{"description":"The NIM models to use.\nFor example:\nmodels:\n  meta/llama-3.1-8b-instruct:\n    image: nvcr.io/nim/meta/llama-3.1-8b-instruct:1.3.3\n    imagePullPolicy: IfNotPresent\n    modelName: meta/llama-3.1-8b-instruct\n    modelVersion: 1.3.3\n    openaiPort: 8000\n    logLevel: DEBUG\n    resources:\n      requests:\n        cpu: 0\n        memory: 0\n      limits:\n        cpu: 0\n        memory: 0\n        nvidia.com/gpu: 1\n      volume:\n        storageClassName: \"standard\"\n        size: \"50Gi\"\n        accessMode: \"ReadWriteOnce\"","default":{}},"helm-values.nim.ngcApiKey":{"description":"The NIM API key to use for accessing the NIM API.","type":"string","default":""},"helm-values.nodeSelector":{"description":"The nodeSelector on Pods tells Kubernetes to schedule Pods on the nodes with matching labels. For more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/).","type":"object"},"helm-values.ollama":{"type":"object","properties":{"debug":{"$ref":"#/$defs/helm-values.ollama.debug"},"dynamicModelLoading":{"$ref":"#/$defs/helm-values.ollama.dynamicModelLoading"},"forceSpreading":{"$ref":"#/$defs/helm-values.ollama.forceSpreading"},"keepAlive":{"$ref":"#/$defs/helm-values.ollama.keepAlive"},"maxConcurrentRequests":{"$ref":"#/$defs/helm-values.ollama.maxConcurrentRequests"},"runnersDir":{"$ref":"#/$defs/helm-values.ollama.runnersDir"}},"additionalProperties":false},"helm-values.ollama.debug":{"description":"If set, debugging mode is enabled.","type":"boolean","default":false},"helm-values.ollama.dynamicModelLoading":{"description":"If set, the model is loaded dynamically and the puller is run in the daemon mode.","type":"boolean","default":false},"helm-values.ollama.forceSpreading":{"description":"If set, Ollama attemts to all GPUs in a node.\nSetting this to true was mentioned in https://github.com/ollama/ollama/issues/5494 to support multiple H100s, but this setting didn't help.","type":"boolean","default":false},"helm-values.ollama.keepAlive":{"description":"Keep models in memory for a long period of time as we don't want end users to hit slowness due to GPU memory loading.","type":"string","default":"96h"},"helm-values.ollama.maxConcurrentRequests":{"description":"The maximun number of ollama requests procssed in parallel.","type":"number","default":0},"helm-values.ollama.runnersDir":{"description":"The path to the directory for ollama runners.","type":"string","default":"/tmp/ollama-runners"},"helm-values.persistentVolume":{"type":"object","properties":{"accessModes":{"$ref":"#/$defs/helm-values.persistentVolume.accessModes"},"enabled":{"$ref":"#/$defs/helm-values.persistentVolume.enabled"},"existingClaim":{"$ref":"#/$defs/helm-values.persistentVolume.existingClaim"},"selector":{"$ref":"#/$defs/helm-values.persistentVolume.selector"},"size":{"$ref":"#/$defs/helm-values.persistentVolume.size"},"storageClassName":{"$ref":"#/$defs/helm-values.persistentVolume.storageClassName"},"volumeBindingMode":{"$ref":"#/$defs/helm-values.persistentVolume.volumeBindingMode"},"volumeName":{"$ref":"#/$defs/helm-values.persistentVolume.volumeName"}},"additionalProperties":false},"helm-values.persistentVolume.accessModes":{"type":"array","items":{"$ref":"#/$defs/helm-values.persistentVolume.accessModes[0]"}},"helm-values.persistentVolume.accessModes[0]":{"type":"string","default":"ReadWriteOnce"},"helm-values.persistentVolume.enabled":{"description":"If true, use a PVC. If false, use emptyDir.","type":"boolean","default":false},"helm-values.persistentVolume.existingClaim":{"description":"If defined, the loader uses the given PVC and does not create a new one. NOTE: PVC must be manually created before the volume is bound.","type":"string"},"helm-values.persistentVolume.selector":{"description":"If defined, the loader used the PVC matched with this selectors. NOTE: PVC must be manually created before the volume is bound. For more information, see [Persistent Volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)\n\nFor example:\nselector:\n matchLabels:\n   release: \"stable\"\n matchExpressions:\n   - { key: environment, operator: In, values: [ dev ] }","type":"object"},"helm-values.persistentVolume.size":{"description":"The size of volume.","type":"string","default":"100Gi"},"helm-values.persistentVolume.storageClassName":{"description":"The name of the storage class for serving a persistent volume.","type":"string","default":"standard"},"helm-values.persistentVolume.volumeBindingMode":{"description":"If defined, the engine uses the given binding-mode for the volume.","type":"string"},"helm-values.persistentVolume.volumeName":{"description":"If defined, the loader Deployment uses the existing PV that has been provisioned in advance.","type":"string"},"helm-values.podAnnotations":{"description":"Optional additional annotations to add to the Deployment Pods.","type":"object"},"helm-values.podSecurityContext":{"description":"Security Context for the inference-manager-engine pod. For more information, see [Configure a Security Context for a Pod or Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).","type":"object","default":{"fsGroup":2000}},"helm-values.preloadedModelIds":{"description":"Optional a list of model IDs to preload. These models are preloaded at the startup time.\n\nFor example:\npreloadedModelIds:\n- google/gemma-2b-it-q4_0\n- sentence-transformers/all-MiniLM-L6-v2-f16","type":"array","items":{}},"helm-values.readinessProbe":{"type":"object","properties":{"enabled":{"$ref":"#/$defs/helm-values.readinessProbe.enabled"},"failureThreshold":{"$ref":"#/$defs/helm-values.readinessProbe.failureThreshold"},"initialDelaySeconds":{"$ref":"#/$defs/helm-values.readinessProbe.initialDelaySeconds"},"periodSeconds":{"$ref":"#/$defs/helm-values.readinessProbe.periodSeconds"},"successThreshold":{"$ref":"#/$defs/helm-values.readinessProbe.successThreshold"},"timeoutSeconds":{"$ref":"#/$defs/helm-values.readinessProbe.timeoutSeconds"}},"additionalProperties":false},"helm-values.readinessProbe.enabled":{"description":"Specify whether to enable the readiness probe.","type":"boolean","default":true},"helm-values.readinessProbe.failureThreshold":{"description":"After a probe fails `failureThreshold` times in a row, Kubernetes considers that the overall check has failed: the container is not ready/healthy/live.","type":"number","default":5},"helm-values.readinessProbe.initialDelaySeconds":{"description":"Number of seconds after the container has started before startup, readiness or readiness probes are initiated.","type":"number","default":3},"helm-values.readinessProbe.periodSeconds":{"description":"How often (in seconds) to perform the probe. Default to 10 seconds.","type":"number","default":10},"helm-values.readinessProbe.successThreshold":{"description":"Minimum consecutive successes for the probe to be considered successful after having failed.","type":"number","default":1},"helm-values.readinessProbe.timeoutSeconds":{"description":"Number of seconds after which the probe times out.","type":"number","default":3},"helm-values.replicaCount":{"description":"The number of replicas for the inference-manager-engine Deployment.","type":"number","default":1},"helm-values.resources":{"description":"Resources to provide to the inference-manager-engine pod. For more information, see [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-Containers/).\n\nFor example:\nrequests:\n  cpu: 10m\n  memory: 32Mi","type":"object","default":{"requests":{"cpu":"1000m","memory":"500Mi"}}},"helm-values.runtime":{"type":"object","properties":{"imagePullPolicy":{"$ref":"#/$defs/helm-values.runtime.imagePullPolicy"},"imagePullSecrets":{"$ref":"#/$defs/helm-values.runtime.imagePullSecrets"},"pullerPort":{"$ref":"#/$defs/helm-values.runtime.pullerPort"},"runtimeImages":{"$ref":"#/$defs/helm-values.runtime.runtimeImages"},"scheduledShutdown":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown"}},"additionalProperties":false},"helm-values.runtime.imagePullPolicy":{"description":"Kubernetes imagePullPolicy for the runtime Statefulset.","type":"string","default":"IfNotPresent"},"helm-values.runtime.imagePullSecrets":{"description":"Kubernetes imagePullSecrets for the runtime Statefulset.","type":"array","default":[],"items":{}},"helm-values.runtime.pullerPort":{"description":"The port number of the puller server for the daemon mode.","type":"number","default":8080},"helm-values.runtime.runtimeImages":{"type":"object","properties":{"ollama":{"$ref":"#/$defs/helm-values.runtime.runtimeImages.ollama"},"triton":{"$ref":"#/$defs/helm-values.runtime.runtimeImages.triton"},"vllm":{"$ref":"#/$defs/helm-values.runtime.runtimeImages.vllm"}},"additionalProperties":false},"helm-values.runtime.runtimeImages.ollama":{"description":"The container image of Ollama.","type":"string","default":"mirror.gcr.io/ollama/ollama:0.6.3-rc0"},"helm-values.runtime.runtimeImages.triton":{"description":"The container image of NVIDIA Triton Server.\nRelease 2.50.0 (https://github.com/triton-inference-server/server/releases/tag/v2.50.0). We might need to recompile models when we upgrade the version of Triton Inference Server and TensortRT-LLM.","type":"string","default":"nvcr.io/nvidia/tritonserver:24.09-trtllm-python-py3"},"helm-values.runtime.runtimeImages.vllm":{"description":"The container image of vLLM.","type":"string","default":"mirror.gcr.io/vllm/vllm-openai:v0.7.3"},"helm-values.runtime.scheduledShutdown":{"type":"object","properties":{"enable":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.enable"},"image":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.image"},"schedule":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.schedule"}},"additionalProperties":false},"helm-values.runtime.scheduledShutdown.enable":{"description":"Specify whether to create a CronJob for shutdown.\nIf enable, The CronJob scales up/down all runtime StatefulSets at the specified time. At scaling-up, the number of replicas will be `.model.default.replicas`","type":"boolean","default":false},"helm-values.runtime.scheduledShutdown.image":{"type":"object","properties":{"name":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.image.name"},"pullPolicy":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.image.pullPolicy"}},"additionalProperties":false},"helm-values.runtime.scheduledShutdown.image.name":{"description":"The container image of kubectl.","type":"string","default":"bitnami/kubectl:latest"},"helm-values.runtime.scheduledShutdown.image.pullPolicy":{"description":"Kubernetes imagePullPolicy for the kubectl.","type":"string","default":"IfNotPresent"},"helm-values.runtime.scheduledShutdown.schedule":{"type":"object","properties":{"scaleDown":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.schedule.scaleDown"},"scaleUp":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.schedule.scaleUp"},"timeZone":{"$ref":"#/$defs/helm-values.runtime.scheduledShutdown.schedule.timeZone"}},"additionalProperties":false},"helm-values.runtime.scheduledShutdown.schedule.scaleDown":{"description":"The schedule of scaling-down, following cron syntax. e.g., \"0 17 * * 5\" # Every Friday at 5 PM","type":"string","default":""},"helm-values.runtime.scheduledShutdown.schedule.scaleUp":{"description":"The schedule of scaling-up, following cron syntax. e.g., \"0 9 * * 1\" # Every Monday at 9 AM","type":"string","default":""},"helm-values.runtime.scheduledShutdown.schedule.timeZone":{"description":"Optional name of the time zone (e.g., \"Etc/UTC\").\nIf empty, the kube-controller-manager interprets schedules relative to its host time zone.","type":"string"},"helm-values.securityContext":{"description":"Security Context for the inference-manager-engine container. For more information, see [Configure a Security Context for a Pod or Container](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).","type":"object","default":{"capabilities":{"drop":["ALL"]},"readOnlyRootFilesystem":true,"runAsNonRoot":true,"runAsUser":1000}},"helm-values.serviceAccount":{"type":"object","properties":{"create":{"$ref":"#/$defs/helm-values.serviceAccount.create"},"name":{"$ref":"#/$defs/helm-values.serviceAccount.name"}},"additionalProperties":false},"helm-values.serviceAccount.create":{"description":"Specifies whether a service account should be created.","type":"boolean","default":true},"helm-values.serviceAccount.name":{"description":"The name of the service account to use. If not set and create is true, a name is generated using the fullname template.","type":"string"},"helm-values.terminationGracePeriodSeconds":{"description":"Optional duration in seconds the pod needs to terminate gracefully. The value zero indicates stop immediately via the kill signal (no opportunity to shut down). If not specified, the default grace period (30 seconds) will be used instead.","type":"string"},"helm-values.tolerations":{"description":"A list of Kubernetes Tolerations, if required.\nFor more information, see [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).\n\nFor example:\ntolerations:\n- key: foo.bar.com/role\n  operator: Equal\n  value: master\n  effect: NoSchedule","type":"array","items":{}},"helm-values.tritonProxyImage":{"type":"object","properties":{"repository":{"$ref":"#/$defs/helm-values.tritonProxyImage.repository"}},"additionalProperties":false},"helm-values.tritonProxyImage.repository":{"description":"The container image name.","type":"string","default":"public.ecr.aws/cloudnatix/llmariner/inference-manager-triton-proxy"},"helm-values.version":{"description":"Override the container image tag to deploy by setting this variable. If no value is set, the chart's appVersion will be used.","type":"string"},"helm-values.vllm":{"type":"object","properties":{"dynamicLoRALoading":{"$ref":"#/$defs/helm-values.vllm.dynamicLoRALoading"},"loggingLevel":{"$ref":"#/$defs/helm-values.vllm.loggingLevel"}},"additionalProperties":false},"helm-values.vllm.dynamicLoRALoading":{"description":"If set, dynamic LoRA adapter loading is enabled.","type":"boolean","default":false},"helm-values.vllm.loggingLevel":{"description":"Logging level of VLLM.","type":"string","default":"ERROR"},"helm-values.volumeMounts":{"description":"Additional volume mounts to add to the inference-manager-engine container. For more information, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/).","type":"array","items":{}},"helm-values.volumes":{"description":"Additional volumes to add to the inference-manager-engine pod. For more information, see [Volumes](https://kubernetes.io/docs/concepts/storage/volumes/).","type":"array","items":{}}}}
