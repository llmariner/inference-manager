syntax = "proto3";

package llmariner.inference.server.v1;

import "api/v1/inference_server.proto";
import "api/v1/inference_server_embeddings.proto";
import "api/v1/inference_server_management.proto";

option go_package = "github.com/llmariner/inference-manager/api/v1";

message HeaderValue {
  repeated string values = 1;
}

message HttpResponse {
  int32 status_code = 1;
  string status = 2;
  map<string, HeaderValue> header = 3;
  // body is empty for server sent events.
  bytes body = 4;
}

message ServerSentEvent {
  bytes data = 1;
  bool is_last_event = 2;
}

message TaskResult {
  string task_id = 1;
  oneof message {
    HttpResponse http_response = 2;
    ServerSentEvent server_sent_event = 3;
  }
}

message ProcessTasksRequest {
  oneof message {
    EngineStatus engine_status = 1;
    TaskResult task_result = 2;
  }
}

message GoAwayRequest {
}

message TaskRequest {
  oneof request {
    llmariner.chat.server.v1.CreateChatCompletionRequest chat_completion = 1;
    llmariner.embeddings.server.v1.CreateEmbeddingRequest embedding = 2;
    ActivateModelRequest model_activation = 3;
    DeactivateModelRequest model_deactivation = 4;
    // go_away is used to signal the worker to disconnect and
    // reconnect to other server.
    GoAwayRequest go_away = 5;
  }
}

message Task {
  string id = 1;

  TaskRequest request = 4;
  map<string, HeaderValue> header = 3;

  // Next ID: 5
}

message ProcessTasksResponse {
  Task new_task = 1;
}

service InferenceWorkerService {
  rpc ProcessTasks(stream ProcessTasksRequest) returns (stream ProcessTasksResponse) {}
}
